{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f534ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "# opencv_python_version = lambda str_version: tuple(map(int, (str_version.split(\".\"))))\n",
    "# assert opencv_python_version(cv.__version__) >= opencv_python_version(\"4.10.0\"), \\\n",
    "#        \"Please install latest opencv-python for benchmark: python3 -m pip install --upgrade opencv-python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e35c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YuNet:\n",
    "    def __init__(self, modelPath, inputSize=[320, 320], confThreshold=0.6, nmsThreshold=0.3, topK=5000, backendId=0, targetId=0):\n",
    "        self._modelPath = modelPath\n",
    "        self._inputSize = tuple(inputSize) # [w, h]\n",
    "        self._confThreshold = confThreshold\n",
    "        self._nmsThreshold = nmsThreshold\n",
    "        self._topK = topK\n",
    "        self._backendId = backendId\n",
    "        self._targetId = targetId\n",
    "\n",
    "        self._model = cv.FaceDetectorYN.create(\n",
    "            model=self._modelPath,\n",
    "            config=\"\",\n",
    "            input_size=self._inputSize,\n",
    "            score_threshold=self._confThreshold,\n",
    "            nms_threshold=self._nmsThreshold,\n",
    "            top_k=self._topK,\n",
    "            backend_id=self._backendId,\n",
    "            target_id=self._targetId)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "    def setBackendAndTarget(self, backendId, targetId):\n",
    "        self._backendId = backendId\n",
    "        self._targetId = targetId\n",
    "        self._model = cv.FaceDetectorYN.create(\n",
    "            model=self._modelPath,\n",
    "            config=\"\",\n",
    "            input_size=self._inputSize,\n",
    "            score_threshold=self._confThreshold,\n",
    "            nms_threshold=self._nmsThreshold,\n",
    "            top_k=self._topK,\n",
    "            backend_id=self._backendId,\n",
    "            target_id=self._targetId)\n",
    "\n",
    "    def setInputSize(self, input_size):\n",
    "        self._model.setInputSize(tuple(input_size))\n",
    "\n",
    "    def infer(self, image):\n",
    "        # Forward\n",
    "        faces = self._model.detect(image)\n",
    "        return np.empty(shape=(0, 5)) if faces[1] is None else faces[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13c13538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid combinations of backends and targets\n",
    "backend_target_pairs = [\n",
    "    [cv.dnn.DNN_BACKEND_OPENCV, cv.dnn.DNN_TARGET_CPU],\n",
    "]\n",
    "\n",
    "def visualize(image, results, box_color=(0, 255, 0), text_color=(0, 0, 255), fps=None):\n",
    "    output = image.copy()\n",
    "    landmark_color = [\n",
    "        (255,   0,   0), # right eye\n",
    "        (  0,   0, 255), # left eye\n",
    "        (  0, 255,   0), # nose tip\n",
    "        (255,   0, 255), # right mouth corner\n",
    "        (  0, 255, 255)  # left mouth corner\n",
    "    ]\n",
    "\n",
    "    if fps is not None:\n",
    "        cv.putText(output, 'FPS: {:.2f}'.format(fps), (0, 15), cv.FONT_HERSHEY_SIMPLEX, 0.5, text_color)\n",
    "\n",
    "    for det in results:\n",
    "        bbox = det[0:4].astype(np.int32)\n",
    "        cv.rectangle(output, (bbox[0], bbox[1]), (bbox[0]+bbox[2], bbox[1]+bbox[3]), box_color, 2)\n",
    "\n",
    "        conf = det[-1]\n",
    "        cv.putText(output, '{:.4f}'.format(conf), (bbox[0], bbox[1]+12), cv.FONT_HERSHEY_DUPLEX, 0.5, text_color)\n",
    "\n",
    "        landmarks = det[4:14].astype(np.int32).reshape((5,2))\n",
    "        for idx, landmark in enumerate(landmarks):\n",
    "            cv.circle(output, landmark, 2, landmark_color[idx], 2)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "model = YuNet(modelPath=\"face_detection_yunet_2023mar.onnx\",\n",
    "                inputSize=[320, 320],\n",
    "                backendId=cv.dnn.DNN_BACKEND_OPENCV,\n",
    "                targetId=cv.dnn.DNN_TARGET_CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a15db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rospy\n",
    "from sensor_msgs.msg import Image\n",
    "from cv_bridge import CvBridge, CvBridgeError\n",
    "\n",
    "rospy.init_node('jupyter')\n",
    "image = rospy.wait_for_message('/io/internal_camera/head_camera/image_rect_color', Image)\n",
    "# convert to OpenCV image\n",
    "bridge = CvBridge()\n",
    "cv_image = bridge.imgmsg_to_cv2(image, \"bgr8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866f3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h, w, _ = image.shape\n",
    "\n",
    "model.setInputSize([w, h])\n",
    "results = model.infer(image)\n",
    "\n",
    "# Print results\n",
    "print('{} faces detected.'.format(results.shape[0]))\n",
    "for idx, det in enumerate(results):\n",
    "    print('{}: {:.0f} {:.0f} {:.0f} {:.0f} {:.0f} {:.0f} {:.0f} {:.0f} {:.0f} {:.0f} {:.0f} {:.0f} {:.0f} {:.0f}'.format(\n",
    "        idx, *det[:-1])\n",
    "    )\n",
    "\n",
    "# Draw results on the input image\n",
    "image = visualize(image, results)\n",
    "cv.namedWindow(\"image\", cv.WINDOW_AUTOSIZE)\n",
    "cv.imshow(\"image\", image)\n",
    "cv.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
